/*
	AI factory; HUGGING FACE
	USES OPENAI LIBRARY
	See README.md

	References:
	- https://huggingface.co/docs/text-generation-inference/reference/api_reference

	Events:
	- lambda-local -l index.js -t 9000 -e events/event-ai-gen-chat-huggingface-lab.json
	- lambda-local -l index.js -t 9000 -e events/event-ai-gen-chat-huggingface-with-attachments-lab.json

	Direct https request:

	curl https://router.huggingface.co/featherless-ai/v1/chat/completions \
    -H "Authorization: Bearer $HF_TOKEN" \
    -H 'Content-Type: application/json' \
    -d '{
        "messages": [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "Describe this image in one sentence."
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": "https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg"
                        }
                    }
                ]
            }
        ],
        "model": "google/medgemma-4b-it",
        "stream": false
    }'

	-- https://huggingface.co/google/medgemma-4b-it?inference_api=true&inference_provider=featherless-ai&language=js

*/

var entityos = require('entityos')
var _ = require('lodash')
var moment = require('moment');

module.exports = 
{
	VERSION: '1.0.0',

	init: function (param)
	{
		entityos.add(
		{
			name: 'ai-gen-util-chat',
			code: function (param)
			{
				//console.log('XAI')
				const settings = entityos.get({scope: '_settings'});
                let aiSettings = _.get(param, 'settings');

				let keyPath = _.get(aiSettings, 'service.keypath');

				let apiKey = _.get(settings, keyPath);
				
				const apiBaseURL = 'https://'
					+ _.get(aiSettings, 'service.host.name', 'api.x.ai')
					+ _.get(aiSettings, 'service.host.basePath', '/v1');

				import('@huggingface/inference').then(function({InferenceClient})
				{
					const hfInference = new InferenceClient(apiKey);

					const maxTokensDefault = _.get(settings, 'ai.defaults.maxtokens', 1000);
					const temperatureDefault = _.get(settings, 'ai.defaults.temperature', 0.7);

					const maxTokens = _.get(param, 'maxTokens', maxTokensDefault);
					const temperature = _.get(param, 'temperature', temperatureDefault);

					let messages = _.get(param, 'messages', {});

					if (messages.system == undefined)
					{
						messages.system = _.get(aiSettings, 'ai.defaults.messages.system');
					}

					if (messages.user == undefined)
					{
						entityos.invoke('util-end', {error: 'No user messages!'});
					}
					else
					{
						let chatMessages =
						[
							{ role: 'system', content: messages.system }
						];

						const attachments = _.get(param, 'attachments');
						
						if (attachments != undefined)
						{
							_.each(attachments, function (attachment)
							{
								if (attachment.base64 != undefined)
								{
									chatMessages.push(
									{
										role: 'user',
										content:
										[
											{ type: 'text', text: messages.user},
											{
												type: 'image_url',
												image_url:
												{
													url: 'data:image/png;base64,' + attachment.base64
												}
											}
										]
									});
								}

								if (attachment.url != undefined)
								{
									chatMessages.push(
									{
										role: 'user',
										content:
										[
											{ type: 'text', text: messages.user},
											{
												type: 'image_url',
												image_url:
												{
													url: attachment.url
												}
											}
										]
									});
								}
							});
						}

						if (_.find(chatMessages, function (chatMessage) {return chatMessage.role == 'user'}) == undefined)
						{
							chatMessages.push({role: 'user', content: messages.user})
						}

						//console.log(aiSettings.model.name)
						//console.log(JSON.stringify(chatMessages));

						let hfProvider = _.get(aiSettings, 'service.provider', 'auto');
						//console.log(hfProvider)
						
						hfInference.chatCompletion({
							provider: hfProvider,
							model: aiSettings.model.name,
							messages: chatMessages,							
							max_tokens: maxTokens,
							temperature: temperature
						})
						.then(function (completion)
						{
							//console.log(completion)
							_.set(param, 'messages.response', completion.choices[0]?.message?.content);

							if (_.get(param, 'onComplete') != undefined)
							{
								entityos._util.onComplete(param);
							}
							else
							{
								entityos.invoke('util-end', param);
							}
						})
						.catch(function (responseError)
						{
							//console.log(responseError);
							//console.log(_.get(responseError, 'httpResponse.body.error'))
							_.set(param, 'messages.response',  '!!: ' + _.get(responseError, 'httpResponse.body.error'));

							if (_.get(param, 'onComplete') != undefined)
							{
								entityos._util.onComplete(param);
							}
							else
							{
								entityos.invoke('util-end', param);
							}
						});
					}
				});
			}
		});
	}
}